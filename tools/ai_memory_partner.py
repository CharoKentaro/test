import streamlit as st
import google.generativeai as genai
import time
from google.api_core import exceptions
import json

# â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
# â˜…â˜…â˜…ã€è¿½æ”¾ã®å„€å¼ã€‘ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼æºã§ã‚ã‚‹ã€ç•°ç‰©ã¯ã€å¬å–šã™ã‚‰ã€ã—ã¾ã›ã‚“ â˜…â˜…â˜…
# â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
# from streamlit_mic_recorder import mic_recorder  <-- ã“ã®ä¸€è¡Œã‚’ã€å®Œå…¨ã«ã€å‰Šé™¤ã—ã¾ã™ã€‚

# --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ---
SYSTEM_PROMPT_TRUE_FINAL = """
# ...ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯å¤‰æ›´ãªã—ï¼‰...
"""

# --- è£œåŠ©é–¢æ•° ---
def dialogue_with_gemini(content_to_process, api_key):
    # ãƒã‚¤ã‚¯æ©Ÿèƒ½ãŒãªããªã£ãŸãŸã‚ã€ã“ã®é–¢æ•°ã¯ã€ã‚·ãƒ³ãƒ—ãƒ«ã«ãªã‚Šã¾ã™ã€‚
    if not content_to_process or not api_key: return None, None
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        processed_text = content_to_process
        original_input_display = processed_text
        with st.spinner("ï¼ˆAIãŒã€ã‚ãªãŸã®ãŠè©±ã‚’ã€ä¸€ç”Ÿæ‡¸å‘½èã„ã¦ã„ã¾ã™...ï¼‰"):
            request_contents = [SYSTEM_PROMPT_TRUE_FINAL, processed_text]
            response = model.generate_content(request_contents)
            ai_response_text = response.text
        return original_input_display, ai_response_text
    except Exception as e:
        st.error(f"AIå‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None, None

# --- ãƒ¡ã‚¤ãƒ³ã®ä»•äº‹ ---
def show_tool(gemini_api_key, localS_object):
    
    localS = localS_object
    prefix = "cc_"
    storage_key_results = f"{prefix}results"

    if st.query_params.get("unlocked") == "true":
        st.session_state[f"{prefix}usage_count"] = 0
        st.query_params.clear()
        retrieved_results = localS.getItem(storage_key_results)
        if retrieved_results:
            st.session_state[storage_key_results] = retrieved_results
        st.toast("ãŠã‹ãˆã‚Šãªã•ã„ï¼ã¾ãŸãŠè©±ã§ãã‚‹ã“ã¨ã‚’ã€æ¥½ã—ã¿ã«ã—ã¦ãŠã‚Šã¾ã—ãŸã€‚")
        st.balloons()

    st.header("â¤ï¸ èªçŸ¥äºˆé˜²ãƒ„ãƒ¼ãƒ«", divider='rainbow')

    if f"{prefix}initialized" not in st.session_state:
        st.session_state[storage_key_results] = localS.getItem(storage_key_results) or []
        st.session_state[f"{prefix}initialized"] = True
    
    if f"{prefix}text_to_process" not in st.session_state: st.session_state[f"{prefix}text_to_process"] = None
    if f"{prefix}last_input" not in st.session_state: st.session_state[f"{prefix}last_input"] = ""
    if f"{prefix}usage_count" not in st.session_state: st.session_state[f"{prefix}usage_count"] = 0

    usage_limit = 3
    is_limit_reached = st.session_state.get(f"{prefix}usage_count", 0) >= usage_limit

    if is_limit_reached:
        st.success("ğŸ‰ ãŸãã•ã‚“ãŠè©±ã„ãŸã ãã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼")
        st.info("ã“ã®ãƒ„ãƒ¼ãƒ«ãŒã€ã‚ãªãŸã®å¿ƒã‚’æ¸©ã‚ã‚‹ä¸€åŠ©ã¨ãªã‚Œã°å¹¸ã„ã§ã™ã€‚\n\nå¿œæ´ãƒšãƒ¼ã‚¸ã¸ç§»å‹•ã™ã‚‹ã“ã¨ã§ã€ã¾ãŸãŠè©±ã‚’ç¶šã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚")
        portal_url = "https://pray-power-is-god-and-cocoro.com/free3/continue.html"
        st.link_button("å¿œæ´ãƒšãƒ¼ã‚¸ã«ç§»å‹•ã—ã¦ã€ãŠè©±ã‚’ç¶šã‘ã‚‹", portal_url, type="primary", use_container_width=True)
    else:
        st.info("ä¸‹ã®å…¥åŠ›æ¬„ã«ã€æ˜”ã®æ¥½ã—ã‹ã£ãŸæ€ã„å‡ºã‚„ã€é ‘å¼µã£ãŸãŠè©±ãªã©ã€ãªã‚“ã§ã‚‚è‡ªç”±ã«ã”å…¥åŠ›ãã ã•ã„ã€‚")
        st.caption(f"ğŸš€ ã‚ã¨ {usage_limit - st.session_state.get(f'{prefix}usage_count', 0)} å›ã€ãŠè©±ã§ãã¾ã™ã€‚")
        
        # â˜…â˜…â˜…ã€è…•ã®æ²»ç™‚ã€‘è‹±é›„ã¯ã€ã‚‚ã¯ã‚„ã€å£°ï¼ˆãƒã‚¤ã‚¯ï¼‰ã‚’ã€å¤±ã„ã€æ–‡å­—ã ã‘ã§ã€èªã‚Šã¾ã™ â˜…â˜…â˜…
        def handle_text_input():
            st.session_state[f"{prefix}text_to_process"] = st.session_state.cc_text
        
        st.text_input(
            "ã“ã“ã«æ–‡ç« ã‚’å…¥åŠ›ã—ã¦Enter...", 
            key=f"{prefix}text", 
            on_change=handle_text_input
        )

    content_to_process = None
    if st.session_state.get(f"{prefix}text_to_process"):
        content_to_process = st.session_state.get(f"{prefix}text_to_process")
        st.session_state[f"{prefix}text_to_process"] = None

    if content_to_process and content_to_process != st.session_state.get(f"{prefix}last_input"):
        st.session_state[f"{prefix}last_input"] = content_to_process
        if not gemini_api_key:
            st.error("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§Gemini APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        else:
            original, ai_response = dialogue_with_gemini(content_to_process, gemini_api_key)
            if original and ai_response:
                st.session_state[f"{prefix}usage_count"] += 1
                st.session_state[storage_key_results].insert(0, {"original": original, "response": ai_response})
                localS.setItem(storage_key_results, st.session_state[storage_key_results])
                st.rerun()
            else:
                st.session_state[f"{prefix}last_input"] = ""

    if st.session_state.get(storage_key_results):
        st.write("---")
        for result in st.session_state[storage_key_results]:
            with st.chat_message("user"):
                st.write(result['original'])
            with st.chat_message("assistant"):
                st.write(result['response'])
        if st.button("ä¼šè©±ã®å±¥æ­´ã‚’ã‚¯ãƒªã‚¢", key=f"{prefix}clear_history"):
            st.session_state[storage_key_results] = []
            st.session_state[f"{prefix}last_input"] = ""
            localS.setItem(storage_key_results, [])
            if f"{prefix}initialized" in st.session_state:
                 del st.session_state[f"{prefix}initialized"]
            st.rerun()
