# ===============================================================
# â˜…â˜…â˜… job_search_tool.py ï¼œæ–°ç€æ¡ˆä»¶ã‚¦ã‚©ãƒƒãƒãƒ£ãƒ¼ãƒ»åŸºæœ¬ç‰ˆï¼ â˜…â˜…â˜…
# ===============================================================
import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# --- Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•° (æ±‚äººãƒœãƒƒã‚¯ã‚¹ç‰¹åŒ–ç‰ˆ) ---
def search_jobs_on_kyujinbox(keywords):
    """
    æ±‚äººãƒœãƒƒã‚¯ã‚¹ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’è¡Œã„ã€çµæœã‚’ãƒªã‚¹ãƒˆã§è¿”ã™ã€‚
    """
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸå½¢å¼ã«å¤‰æ›
    search_url = f"https://xn--pckua2a7gp15o89zb.com/kw-{'+'.join(keywords.split())}"
    headers = {
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾ç­–ã‚’å›é¿ã™ã‚‹ãŸã‚ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        with st.spinner(f"ã€Œ{keywords}ã€ã®æ¡ˆä»¶ã‚’æ±‚äººãƒœãƒƒã‚¯ã‚¹ã§æ¤œç´¢ä¸­..."):
            # ã‚µã‚¤ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹
            response = requests.get(search_url, headers=headers, timeout=10)
            response.raise_for_status() # HTTPã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹
            
            # HTMLã‚’è§£æ
            soup = BeautifulSoup(response.content, "html.parser")
            
            # æ±‚äººæƒ…å ±ãŒæ›¸ã‹ã‚Œã¦ã„ã‚‹ã‚«ãƒ¼ãƒ‰è¦ç´ ã‚’ã™ã¹ã¦å–å¾—
            # â€»ã‚µã‚¤ãƒˆã®æ§‹é€ ãŒå¤‰ã‚ã‚‹ã¨ã€ã“ã“ã®ã‚¯ãƒ©ã‚¹åãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹
            job_cards = soup.find_all("div", class_=lambda c: c and ("result_card_" in c or "p-job_list-item" in c))

            results = []
            if not job_cards:
                st.warning("æ±‚äººæƒ…å ±ã‚’è¦‹ã¤ã‘ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ã‚µã‚¤ãƒˆã®æ§‹é€ ãŒå¤‰æ›´ã•ã‚ŒãŸã‹ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åˆã†æ±‚äººãŒãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                return []

            # å„ã‚«ãƒ¼ãƒ‰ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            for card in job_cards:
                title_tag = card.find("h3", class_=lambda c: c and "heading_title" in c)
                company_tag = card.find("span", class_=lambda c: c and "text_company" in c)
                link_tag = card.find("a", href=True)

                title = title_tag.text.strip() if title_tag else "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
                company = company_tag.text.strip() if company_tag else "ä¼šç¤¾åä¸æ˜"
                
                # URLãŒç›¸å¯¾ãƒ‘ã‚¹ï¼ˆ/ã‹ã‚‰å§‹ã¾ã‚‹ï¼‰ã®å ´åˆã€ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’è£œå®Œã—ã¦çµ¶å¯¾ãƒ‘ã‚¹ã«ã™ã‚‹
                url = "https://xn--pckua2a7gp15o89zb.com" + link_tag['href'] if link_tag and link_tag['href'].startswith('/') else link_tag['href'] if link_tag else "URLä¸æ˜"

                results.append({
                    "æ¡ˆä»¶ã‚¿ã‚¤ãƒˆãƒ«": title,
                    "ä¼šç¤¾å": company,
                    "è©³ç´°URL": url,
                })
        
        return results

    except requests.exceptions.RequestException as e:
        st.error(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None
    except Exception as e:
        st.error(f"æƒ…å ±ã®å–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None


# --- ãƒ¡ã‚¤ãƒ³é–¢æ•° ---
def show_tool(gemini_api_key): # APIã‚­ãƒ¼ã¯å°†æ¥ã®æ‹¡å¼µç”¨ã«å¼•æ•°ã¯æ®‹ã—ã¾ã™
    st.header("ğŸ’¼ æ–°ç€æ¡ˆä»¶ã‚¦ã‚©ãƒƒãƒãƒ£ãƒ¼", divider='rainbow')
    st.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ã€ã€Œæ±‚äººãƒœãƒƒã‚¯ã‚¹ã€ã‹ã‚‰æ–°ã—ã„ä»•äº‹ã®æ©Ÿä¼šã‚’è¦‹ã¤ã‘ã¾ã—ã‚‡ã†ã€‚")

    prefix = "job_search_"
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
    if f"{prefix}results" not in st.session_state:
        st.session_state[f"{prefix}results"] = None

    # --- å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ  ---
    with st.form("job_search_form"):
        keywords = st.text_input(
            "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
            placeholder="ä¾‹: Python æ¥­å‹™åŠ¹ç‡åŒ– æ¥­å‹™å§”è¨—",
            help="è¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã£ã¦ãã ã•ã„ã€‚"
        )
        submitted = st.form_submit_button("ğŸ” ã“ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã™ã‚‹", type="primary", use_container_width=True)

    if submitted:
        if not keywords:
            st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            search_results = search_jobs_on_kyujinbox(keywords)
            # æ¤œç´¢çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
            st.session_state[f"{prefix}results"] = search_results
            if search_results:
                 st.success(f"{len(search_results)} ä»¶ã®æ¡ˆä»¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼")
            time.sleep(1)
            st.rerun()

    # --- çµæœè¡¨ç¤º ---
    if st.session_state[f"{prefix}results"] is not None:
        st.divider()
        st.subheader("ğŸ“Š æ¤œç´¢çµæœ")
        
        results = st.session_state[f"{prefix}results"]
        if not results:
            st.info("è©²å½“ã™ã‚‹æ¡ˆä»¶ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¤‰ãˆã¦è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")
        else:
            # Pandas DataFrameã§è¦‹ã‚„ã™ãè¡¨ç¤º
            df = pd.DataFrame(results)
            st.dataframe(
                df,
                use_container_width=True,
                hide_index=True,
                # URLã‚’ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ã«ã™ã‚‹è¨­å®šï¼ˆå°†æ¥çš„ãªæ”¹å–„æ¡ˆï¼‰
                # column_config={"è©³ç´°URL": st.column_config.LinkColumn("è©³ç´°URL")}
            )

        if st.button("æ¤œç´¢çµæœã‚’ã‚¯ãƒªã‚¢", key=f"{prefix}clear_results"):
            st.session_state[f"{prefix}results"] = None
            st.rerun()
